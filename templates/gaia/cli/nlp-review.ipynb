{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "436aa5a8-0580-4c85-9023-ca57d13f68d9",
   "metadata": {},
   "source": [
    "# NLP review\n",
    "\n",
    "The following topics are considered:\n",
    "\n",
    "## Word embeddings\n",
    "1. term/frequency methods\n",
    "2. Dimensional reduction\n",
    "3. Latent Semantic Analysis\n",
    "4. Latent Dirichlet Allocation\n",
    "\n",
    "## Sequence embeddings\n",
    "1. Seq2Seq\n",
    "2. BERT, transformers\n",
    "\n",
    "## Graph networks\n",
    "1. Adjacency and degree representations\n",
    "2. Homogeneous nodes\n",
    "3. Heterogeneous nodes \n",
    "4. Graph embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28c9fbc-9b40-4882-99dc-67bb4a9b1c39",
   "metadata": {},
   "source": [
    "## Term/Frequency methods (tf-idf, SVD of of Doc-Term matrix)\n",
    "\n",
    "Traditional [word](https://arxiv.org/abs/1301.3781) and [paragraph](https://cs.stanford.edu/~quocle/paragraph_vector.pdf) embedding preserve little to no sentence structure, relying instead on word proximity and occurrence statistics.\n",
    "\n",
    "As a baseline, we'll use [LSA](https://en.wikipedia.org/wiki/Latent_semantic_analysis) and compare it to [LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation).\n",
    "\n",
    "The hyper parameters of the model influence the stochastically determined mapping from written English to a latent space of thoughts. This is similar to how [Uniform Manifold Approximation and Projection](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html) (UMAP) deforms the manifold based on nearest neighbor information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c85826-fb8a-451b-a2a2-1cc88c17b888",
   "metadata": {},
   "source": [
    "## Sequential methods (seq2seq, transformer)\n",
    "\n",
    "This work attempts to be consistent with the language and intuitions established by [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) published in 2014 by Ilya Sutskever, Oriol Vinyals, Quoc V. Le.\n",
    "\n",
    "Seq2seq turns one sequence into another sequence (sequence transformation). It does so by use of a pair recurrent neural networks (RNNs): the encoder which turns a portion of the sequential input into a latent vector. The inputs of the encoder are current sequential data and a recursive state-vector not unlike state-space methods for time-series model estimation. \n",
    "\n",
    "\n",
    "\n",
    "### Topics associated with seq2seq\n",
    "- [1997: Long Short-Term Memory](http://www.bioinf.jku.at/publications/older/2604.pdf).  LSTM first introduced in 1997 by Sepp Hochreiter and Jurgen Schmidhuber, Solves the vanishing gradient problem  with the concept of \"constant error carrousel\", CEC. Memory cells and gate units are are clearly motivated and described.\n",
    "\n",
    "- Attention: The input to the decoder is a single vector which stores the entire context. Attention allows the decoder to look at the input sequence selectively. [Tensor2Tensor](https://github.com/tensorflow/tensor2tensor) is the place to start.\n",
    "\n",
    "- Beam Search: Instead of picking the single output (word) as the output, multiple highly probable choices are retained, structured as a tree (using a Softmax on the set of attention scores[6]). Average the encoder states weighted by the attention distribution\n",
    "\n",
    "- Bucketing: Variable-length sequences are possible because of padding with 0s, which may be done to both input and output. However, if the sequence length is 100 and the input is just 3 items long, expensive space is wasted. Buckets can be of varying sizes and specify both input and output lengths.\n",
    "\n",
    "- [2017: LSTM: A Search Space Odyssey](https://arxiv.org/pdf/1503.04069.pdf) by Greff, Schmidhuber, et. al. Analyzes 8 LSTM methods, good overview before the transformer explosion a year later.\n",
    "\n",
    "Training typically uses a cross-entropy loss function, whereby one output is penalized to the extent that the probability of the succeeding output is less than 1.[6]\n",
    "\n",
    "\n",
    "- [2014: Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) by Ilya Sutskever, Oriol Vinyals, Quoc V. Le, 2014. Uses LSTM to map sequences to a fixed length and another LSTM  to decode the target sequence of the vector.\n",
    "\n",
    "\n",
    "[2016: Sequence-to-Sequence Learning as Beam-Search Optimization](https://arxiv.org/abs/1606.02960) by Sam Wiseman and Alexander M. Rush, 2016. Alternative to attention.\n",
    "\n",
    "[2018: Learning Neural Templates for Text Generation](https://arxiv.org/abs/1808.10122) 2018 by Sam Wiseman, Stuart M. Shieber, Alexander M. Rush. Because English relies on conventions for syntax and typical syntax pattern, Wiseman et. al present a scheme that accounts for, and takes advantage of, template based formatting for input vectors.\n",
    "\n",
    "- [2018: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
    "](https://arxiv.org/abs/1810.04805): Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova.\n",
    "  - [Chris McCommick BERT break down](https://www.youtube.com/watch?v=FKlPCK1uFrc&list=PLam9sigHPGwOBuH4_4fr-XvDbe5uneaf6)\n",
    "\n",
    "- [2018: Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf) The transformer model, perfected.\n",
    " - [walk-thru](http://nlp.seas.harvard.edu/annotated-transformer/) The 'annotated transformer' at Harvard.\n",
    "\n",
    "\n",
    "### Graph networks\n",
    "\n",
    "The [TextRank](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf) algorithm by Rada Mihalcea and Paul Tarau is the starting point for the theory behind Gaia NLP. Like TextRank, Gaia NLP summarizes and contextualizes a corpus using graph based, unsupervised techniques. \n",
    "\n",
    "See Chapter 5 \"Geometric Deep Learning Models\" of Bronstein et. al's [Geometric Deep Learning\n",
    "Grids, Groups, Graphs, Geodesics, and Gauges](https://arxiv.org/pdf/2104.13478.pdf) for explainations of how graph neural networks are used in conjunction with sequence based models like LSTM and transformers with attention.\n",
    "\n",
    "Prakhar Mishra's short walk-thrus are good:\n",
    " - [TextRank: Bringing Order into Texts](https://www.youtube.com/watch?v=2l6Fa767kEw)\n",
    " - [Node2Vec: Scalable Feature Learning for Networks | ML with Graphs ](https://www.youtube.com/watch?v=LpwGZG5j_q0)\n",
    " - [DEEPWALK: Online Learning of Social Representations](https://www.youtube.com/watch?v=-uJL_ANy1jc)\n",
    "\n",
    "### References\n",
    "\n",
    "- Socher, Bauer, Manning and Ng.Their paper [Parsing with Compositional Vector Grammars](https://nlp.stanford.edu/pubs/SocherBauerManningNg_ACL2013.pdf) crystallized the vague notion of augmenting tokens with parts of speech and using this information to create a latent space that honors syntax and meaning and not just meaning itself. \n",
    "\n",
    "\n",
    "- 1977 beam search paper, [Word Reordering and a Dynamic\n",
    "Programming Beam Search Algorithm for\n",
    "Statistical Machine Translation](https://aclanthology.org/J03-1005.pdf)  by Christoph Tillmann and Hermann Ney. The paper introduces beam search- a statistical machine translation\n",
    "based on [dynamic programming](https://en.wikipedia.org/wiki/Dynamic_programming). This is a good rosetta stone algorithm between the fields of NLP and [computational genomics](https://en.wikipedia.org/wiki/Computational_genomics).\n",
    "\n",
    "\n",
    "- Xi Rong's [word2vec Parameter Learning Explained](https://arxiv.org/abs/1411.2738) walks through a toy model that clearly explains bag of words, skip-gram, hierarchical soft-max and negative sampling.\n",
    "\n",
    "\n",
    "## Socher et. al: \n",
    "\n",
    "- [Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions](https://aclanthology.org/D11-1014.pdf)\n",
    "  - [Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks](https://arxiv.org/abs/1503.00075)\n",
    "  - [Glove: Global Vectors for Word Representation](https://aclanthology.org/D14-1162.pdf)[Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](https://aclanthology.org/D13-1170.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c295d32-c3fe-419f-bc06-bd386545d013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
